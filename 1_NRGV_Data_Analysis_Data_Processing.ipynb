{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NRGV Data Analyst Interview Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Identify the best location to install a solar array considering the following states:<br>\n",
    "<ul>\n",
    "    <li>California</li>\n",
    "    <li>Texas</li>\n",
    "    <li>Alabama</li>\n",
    "    <li>Florida</li>\n",
    "    <li>Maine</li>\n",
    "    <li>Colorado</li>\n",
    "</ul>\n",
    "2) Present your selected location using graphics that best convey your results for senior leadership<br>\n",
    "3) For the location you have identified, calculate the forecasted power that a solar array would produce in megawatts<br>\n",
    "<ul>\n",
    "    <li>Present your calculations in a graph that shows production monthly for a year</li>\n",
    "    <li>What other graphics can provide insight into the forecasted power production?</li>\n",
    "</ul>\n",
    "4) Extra credit ideas (you can be creative and find other ways to analyze this data):<br>\n",
    "<ul>\n",
    "    <li>Plot the candidate sites on a map with an overlay of their forecasted production</li>\n",
    "    <li>Automate the download, unziping and ingest of study data to increase the number of states you analyze</li>\n",
    "    <li>What impact did weather have on power production rates?</li>\n",
    "</ul>\n",
    "Data: https://www.nrel.gov/grid/solar-power-data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work - Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Could test for count of underscores in file name to chack for additional encodings<br>\n",
    "2) Using LAT/LONG and Date, determine sunset/sunrise; progamaticaly remove records when the sun is not out (from sunset to sunrise).<br>\n",
    "2a) Eliminate these records during the injest stage -Document time-delta vs lines of code vs reduced records.<br>\n",
    "3) Automate the detection of new state in dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>NREL's Solar Power Data for Integration Studies are synthetic solar photovoltaic (PV) power plant data points for the United States representing the year 2006.</li>\n",
    "    <li>NREL The data are intended for use by energy professionals—such as transmission planners, utility planners, project developers, and university researchers—who perform solar integration studies and need to estimate power production from hypothetical solar plants.</li>\n",
    "    <li> The Solar Power Data for Integration Studies consist of 1 year (2006) of 5-minute solar power and hourly day-ahead forecasts for approximately 6,000 simulated PV plants.</li>\n",
    "    <li>Solar power plant locations were determined based on the capacity expansion plan for high-penetration renewables in Phase 2 of the Western Wind and Solar Integration Study and the Eastern Renewable Generation Integration Study.</li>\n",
    "    <li>NREL generated the 5-minute data set using the Sub-Hour Irradiance Algorithm. \n",
    "    <li>The day-ahead solar forecast data for locations in the western United States were generated by 3TIER based on numerical weather predication simulations for Phase 1 of the Western Wind and Solar Integration Study. \n",
    "    <li>NREL generated the day-ahead solar forecast data in eastern U.S. locations using the Weather Research and Forecasting model.</li>\n",
    "    <li>The data are for specific years and should not be assumed to be representative of typical radiation levels for a site.</li>\n",
    "    <li>These data should not generally be used for site-specific project development work.</li>\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Present your selected location using graphics that best convey your results for senior leadership</li>\n",
    "    <li>For the location you have identified, calculate the forecasted power that a solar array would produce in megawatts</li>\n",
    "    <li>Present your calculations in a graph that shows production monthly for a year</li>\n",
    "    <li>What other graphics can provide insight into the forecasted power production?</li>\n",
    "        <li>Extra credit ideas (you can be creative and find other ways to analyze this data):</li><br>\n",
    "            <li>Plot the candidate sites on a map with an overlay of their forecasted production</li>\n",
    "            <li> Automate the download, unziping and ingest of study data to increase the number of states you analyze</li>\n",
    "            <li>What impact did weather have on power production rates?</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Create State directory structure - pull data from NREL.</li>\n",
    "    <li>Create SWSD Directory Structure - Create SWSD Summary Data by state.</li>\n",
    "    <li>Write SWSD_Summary_Records_Statistics Log file.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req     # DOWNLOADING URL VIA URL\n",
    "import zipfile             # DEALING WITH ZIP COMPRESSED FILE\n",
    "from io import BytesIO     # READING FILE FROM BUFFER\n",
    "import os                  # DIRECTORY CREATION\n",
    "import pandas as pd        # PANDAS DATAFRAME\n",
    "import datetime            # DATE AND TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DICTIONARY OF STATES TO BE PROCESSED\n",
    "states_dict = {\n",
    "    \"Alabama\":\"al\",\n",
    "    \"Arizona\":\"az\",\n",
    "    \"California\":\"ca\",\n",
    "    \"Colorado\":\"co\",\n",
    "    \"Florida\":\"fl\",\n",
    "    \"Maine\":\"me\",\n",
    "    \"New Mexico\":\"nm\",\n",
    "    \"Texas\":\"tx\"    \n",
    "}\n",
    "\n",
    "# PULL DATA FROM NREL \n",
    "pull_data_from_nrel = False\n",
    "\n",
    "# BASE URL OF STATE-WISE SOLAR POWER DATE\n",
    "data_url = \"https://www.nrel.gov/grid/assets/downloads/_-pv-2006.zip\"\n",
    "\n",
    "# PATH TO PLACE DATA\n",
    "data_path = 'C://Users//bruce//Documents//Data//NRGV'\n",
    "\n",
    "# PATH TO SUMMARY DATA FILES\n",
    "smry_data_path = 'C://Users//bruce//Documents//Data//SWSD_SUMMARY_DATA'\n",
    "\n",
    "# PULL DATA\n",
    "pull_data=False\n",
    "\n",
    "# state-wise solar power data (.csv files) from the Solar Integration Studies \n",
    "# DataType (string), Latitude (float), Longitude (float), WeatherYear (int), PVType (string), CapacityMW (int)\n",
    "# TimeIntervalMin (int), LocalTime (datetime), Power(MW) (float)\n",
    "\n",
    "\n",
    "swsp_features = ['LocalTime','Power(MW)','State','DataType','Latitude','Longitude','WeatherYear','PVType',\\\n",
    "                 'CapacityMW','TimeIntervalMin']\n",
    "\n",
    "# SWSD SUMMARY RECORDS\n",
    "swsd_summary_records_path_filename = smry_data_path + \"//SWSD_Summary_Records_Statistics.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Functions (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    \n",
    "    # DIRECTORY IS CREATED IF IT DOESN'T CURRENTLY EXIST\n",
    "    if not os.path.exists(path):\n",
    "        \n",
    "            # DIRECTORY IS CREATED BECAUSE IT DOESN'T CURRENTLY EXIST\n",
    "            os.makedirs(path)\n",
    "            \n",
    "            print(\"The \" + path + \" directory was created.\")\n",
    "            \n",
    "    else:\n",
    "        print(\"The \" + path + \" directory already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all_files(path):\n",
    "    \n",
    "    # DELETE ALL FILES IN DIRECTORY\n",
    "    for file in os.listdir(path):\n",
    "        os.remove(os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir_and_load_data(pull_data=pull_data_from_nrel):\n",
    "    # https://www.geeksforgeeks.org/how-to-create-directory-if-it-does-not-exist-using-python/\n",
    "\n",
    "    # DETERMINE IF THE data_path EXISTS, IF NOT THE DIRECTORY IS CREATED\n",
    "\n",
    "    # ITERATE THROUGH THE STATES_DICT \n",
    "    for state, state_abrev in states_dict.items():\n",
    "    \n",
    "        # CREATE A VARIABLE, WHICH IS THE DATA_PATH AND THE STATE IN THE STATES_DICT \n",
    "        path_and_dir = data_path + \"\\\\\" + state_abrev\n",
    "\n",
    "        # DIRECTORY IS CREATED IF IT DOESN'T CURRENTLY EXIST\n",
    "        if not os.path.exists(path_and_dir):\n",
    "\n",
    "            # DIRECTORY IS CREATED BECAUSE IT DOESN'T CURRENTLY EXIST\n",
    "            os.makedirs(path_and_dir)\n",
    "            \n",
    "            if pull_data == True:\n",
    "                \n",
    "                # TEST FOR FILE COUNT IN DIRECTORY, PULL DATA ONLY IF DIRECTORY IS EMPTY\n",
    "                if len(os.listdir(temp_path)) == 0:\n",
    "                \n",
    "                    # LOAD DATA\n",
    "                    load_data(state, state_abrev)\n",
    "        \n",
    "        else:\n",
    "            print(\"The \" + state + \" directory already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(state, state_abrev):\n",
    "    \n",
    "    # CREATE A VARIABLE, WHICH IS THE DATA_PATH AND THE STATE IN THE STATES_DICT \n",
    "    path_and_dir = data_path + \"\\\\\" + state_abrev\n",
    "    \n",
    "    # IN THE DATA_PATH VARIABLE, REPLACE THE UNDERSCORE WITH THE CURRENT VALUE IN THE STATES_DICT, CREATE VARIABLE \n",
    "    #print(data_url.replace(\"_\",state_abrev))\n",
    "    state_zip_data_url = data_url.replace(\"_\",state_abrev)\n",
    "    print(\"Currently working \" + state_zip_data_url)\n",
    "    \n",
    "    # BUFFER THE DATA\n",
    "    state_zip_data = req.get(state_zip_data_url)\n",
    "    \n",
    "    # READING FILE FROM BUFFER INTO VARIABLE\n",
    "    ziped_file = zipfile.ZipFile(BytesIO(state_zip_data.content))\n",
    "    \n",
    "    # EXTRACT BUFFERED DATA INTO DIRECTORY\n",
    "    ziped_file.extractall(path_and_dir)\n",
    "    \n",
    "    # PRINT STATE COMPLETED\n",
    "    print(\"Completed downloading data for the state of \" + state + \".\")\n",
    "    \n",
    "    # CLEAR OUT VARIABLES\n",
    "    del([path_and_dir,state_zip_data_url,state_zip_data,ziped_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findall(p, s):\n",
    "    # YIELDS A LIST OF ALL THE POSITION INDEX OF THE PATTERN (p) WITHIN THE STRING (s)\n",
    "    i = s.find(p)\n",
    "    while i != -1:\n",
    "        yield i\n",
    "        i = s.find(p, i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_index(to_find, string):\n",
    "    # PRODUCES A LIST OF POSITION INDEX (ZERO BASED) OF to_find WITHIN A STRING\n",
    "    return [(i, string[i:i+1]) for i in findall(to_find, string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_file_metadata_variables(feature_pos_index, file_string):\n",
    "    \n",
    "    # DECOMPOSE FILE NAME FOR META DATA, POPULATE META DATA INTO VARIABLES \n",
    "    current_pos_in_string = 0\n",
    "    current_pos_in_loop = 1\n",
    "    \n",
    "    # DEFINE GLOBAL SCOPE VARIABLES\n",
    "    global DataType_var\n",
    "    global Latitude_var\n",
    "    global Longitude_var\n",
    "    global WeatherYear_var\n",
    "    global PVType_var\n",
    "    global CapacityMW_var\n",
    "    global TimeIntervalMin_var\n",
    "      \n",
    "    for x in feature_pos_index:\n",
    "        \n",
    "        for y in x:\n",
    "            \n",
    "            if isinstance(y, int) == True:\n",
    "\n",
    "                if current_pos_in_loop == 1:\n",
    "                    DataType_var = file_string[current_pos_in_string:y]\n",
    "                elif current_pos_in_loop == 2:\n",
    "                    Latitude_var = file_string[current_pos_in_string:y]\n",
    "                elif current_pos_in_loop == 3:\n",
    "                    Longitude_var = file_string[current_pos_in_string:y]\n",
    "                elif current_pos_in_loop == 4:\n",
    "                    WeatherYear_var = file_string[current_pos_in_string:y]\n",
    "                elif current_pos_in_loop == 5:\n",
    "                    PVType_var = file_string[current_pos_in_string:y]\n",
    "                elif current_pos_in_loop == 6:\n",
    "                    CapacityMW_var = file_string[current_pos_in_string:y-2]\n",
    "                elif current_pos_in_loop == 7:\n",
    "                    TimeIntervalMin_var = file_string[current_pos_in_string:y]\n",
    "\n",
    "\n",
    "                current_pos_in_string = y + 1\n",
    "                current_pos_in_loop = current_pos_in_loop + 1\n",
    "            \n",
    "    #print([DataType_var,Latitude_var,Longitude_var,WeatherYear_var,PVType_var,CapacityMW_var,TimeIntervalMin_var])\n",
    "    return DataType_var,Latitude_var,Longitude_var,WeatherYear_var,PVType_var,CapacityMW_var,TimeIntervalMin_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create State Data Directory Structures and Pull Data From National Renewable Energy Laboratory (NREL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Alabama directory already exists.\n",
      "The Arizona directory already exists.\n",
      "The California directory already exists.\n",
      "The Colorado directory already exists.\n",
      "The Florida directory already exists.\n",
      "The Maine directory already exists.\n",
      "The New Mexico directory already exists.\n",
      "The Texas directory already exists.\n"
     ]
    }
   ],
   "source": [
    "create_dir_and_load_data(pull_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SWSD Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The C://Users//bruce//Documents//Data//SWSD_SUMMARY_DATA directory already exists.\n"
     ]
    }
   ],
   "source": [
    "# CREATE SWSD DATA SUMMARY DIRECTORY\n",
    "create_dir(smry_data_path)\n",
    "SWSD = pd.DataFrame(columns=swsp_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data - Create SWSD Summary Data by state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The process of Summary Data has start for the state of AL.\n",
      "The process started at: 2022-10-16 06:42:52.173001 \n",
      " The process finished at: 2022-10-16 07:05:28.366052 \n",
      " The processing elapsed time was: 0:22:36.193051 \n",
      " The State being processed is: AL. \n",
      " The total number of csv files to be processed is: 411 \n",
      " There are 16801680 records in the file. \n",
      " The AL_SWSD_SUMMARY_DATA has been written. \n",
      "\n",
      "The process of Summary Data has start for the state of AZ.\n",
      "The process started at: 2022-10-16 07:06:06.486762 \n",
      " The process finished at: 2022-10-16 07:41:48.187274 \n",
      " The processing elapsed time was: 0:35:41.700512 \n",
      " The State being processed is: AZ. \n",
      " The total number of csv files to be processed is: 513 \n",
      " There are 20971440 records in the file. \n",
      " The AZ_SWSD_SUMMARY_DATA has been written. \n",
      "\n",
      "The process of Summary Data has start for the state of CA.\n",
      "The process started at: 2022-10-16 07:42:37.098666 \n",
      " The process finished at: 2022-10-16 11:00:09.074094 \n",
      " The processing elapsed time was: 3:17:31.975428 \n",
      " The State being processed is: CA. \n",
      " The total number of csv files to be processed is: 1,215 \n",
      " There are 49669200 records in the file. \n",
      " The CA_SWSD_SUMMARY_DATA has been written. \n",
      "\n",
      "The process of Summary Data has start for the state of CO.\n",
      "The process started at: 2022-10-16 11:02:03.540870 \n",
      " The process finished at: 2022-10-16 11:11:32.164880 \n",
      " The processing elapsed time was: 0:09:28.624010 \n",
      " The State being processed is: CO. \n",
      " The total number of csv files to be processed is: 264 \n",
      " There are 10792320 records in the file. \n",
      " The CO_SWSD_SUMMARY_DATA has been written. \n",
      "\n",
      "The process of Summary Data has start for the state of FL.\n",
      "The process started at: 2022-10-16 11:11:57.104399 \n",
      " The process finished at: 2022-10-16 18:18:36.022670 \n",
      " The processing elapsed time was: 7:06:38.918271 \n",
      " The State being processed is: FL. \n",
      " The total number of csv files to be processed is: 1,779 \n",
      " There are 72725520 records in the file. \n",
      " The FL_SWSD_SUMMARY_DATA has been written. \n",
      "\n",
      "The process of Summary Data has start for the state of ME.\n",
      "The process started at: 2022-10-16 18:21:26.327650 \n",
      " The process finished at: 2022-10-16 18:22:09.189936 \n",
      " The processing elapsed time was: 0:00:42.862286 \n",
      " The State being processed is: ME. \n",
      " The total number of csv files to be processed is: 69 \n",
      " There are 2820720 records in the file. \n",
      " The ME_SWSD_SUMMARY_DATA has been written. \n",
      "\n",
      "The process of Summary Data has start for the state of NM.\n",
      "The process started at: 2022-10-16 18:22:15.718580 \n",
      " The process finished at: 2022-10-16 18:23:14.470880 \n",
      " The processing elapsed time was: 0:00:58.752300 \n",
      " The State being processed is: NM. \n",
      " The total number of csv files to be processed is: 81 \n",
      " There are 3311280 records in the file. \n",
      " The NM_SWSD_SUMMARY_DATA has been written. \n",
      "\n",
      "The process of Summary Data has start for the state of TX.\n",
      "The process started at: 2022-10-16 18:23:22.333877 \n",
      " The process finished at: 2022-10-16 18:27:21.331280 \n",
      " The processing elapsed time was: 0:03:58.997403 \n",
      " The State being processed is: TX. \n",
      " The total number of csv files to be processed is: 168 \n",
      " There are 6841560 records in the file. \n",
      " The TX_SWSD_SUMMARY_DATA has been written. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ITERATE states_dict\n",
    "for state, state_abv in states_dict.items():\n",
    "    \n",
    "    # SET STATE VARIABLE TO \n",
    "    State_var = state_abv.upper()\n",
    "    \n",
    "    print(\"The process of Summary Data has start for the state of {}.\".format(State_var))\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # READ FILENAMES IN STATE DIRECTORY\n",
    "    dir_list = os.listdir(data_path + '\\\\' + state_abv)\n",
    "    \n",
    "    csv_count = len(dir_list)\n",
    "\n",
    "    # 1) ITERATE THROUGH FILES (file_string) IN DIRECTORY (dir_list)\n",
    "    #   2) DECOMPOSE FILE NAME (file_string) FOR META DATA, POPULATE META DATA INTO VARIABLES\n",
    "    #      3) POPULATE DATAFRAME WITH FILE_NAME META DATA VARIABLES AND FILE CONTENTS\n",
    "    #         4) AT THE CONCLUSION OF PROCESSING ALL THE CSV FILES FOR A STATE, WRITE A SUMMARY FILE\n",
    "\n",
    "    # 1) ITERATE THROUGH FILES (file_string) IN DIRECTORY (dir_list)\n",
    "    for swsd_csv_file_string in dir_list:\n",
    "        \n",
    "        \n",
    "        #    2) DECOMPOSE FILE NAME (swsd_csv_file_string) FOR META DATA, POPULATE META DATA INTO VARIABLES\n",
    "        feature_pos_index = get_pos_index('_', swsd_csv_file_string)\n",
    "\n",
    "        DataType_var,Latitude_var,Longitude_var,WeatherYear_var,PVType_var,CapacityMW_var,\\\n",
    "            TimeIntervalMin_var = populate_file_metadata_variables(feature_pos_index, swsd_csv_file_string)\n",
    "        \n",
    "        #   3) POPULATE DATAFRAME WITH FILE_NAME META DATA VARIABLES AND FILE CONTENTS\n",
    "        \n",
    "        # SUBSTANTIATE path_filename WITH THE CURRENT DIRECTORY AND CSV FILE NAME\n",
    "        path_filename = data_path + '\\\\' + State_var + '\\\\' + swsd_csv_file_string\n",
    "        \n",
    "        # READ CSV\n",
    "        temp_df = pd.read_csv(path_filename, header=0)\n",
    "        \n",
    "        # POPULATE FEATURES BASED UPON CURRENT VARIABLES\n",
    "        temp_df['State'] = State_var\n",
    "        temp_df['DataType'] = DataType_var\n",
    "        temp_df['Latitude'] = Latitude_var\n",
    "        temp_df['Longitude'] = Longitude_var\n",
    "        temp_df['WeatherYear'] = WeatherYear_var\n",
    "        temp_df['PVType'] = PVType_var\n",
    "        temp_df['CapacityMW'] = CapacityMW_var\n",
    "        temp_df['TimeIntervalMin'] = TimeIntervalMin_var\n",
    "        \n",
    "        #print(temp_df.head())\n",
    "        \n",
    "        # APPEND temp_df TO SWSD DATAFRAM\n",
    "        SWSD = SWSD.append(temp_df)\n",
    "        \n",
    "        \n",
    "    # 4) AT THE CONCLUSION OF PROCESSING ALL THE CSV FILES FOR A STATE, WRITE A SUMMARY FILE\n",
    "    \n",
    "    stop_time = datetime.datetime.now()\n",
    "    processing_elapsed_time = stop_time - start_time\n",
    "    \n",
    "    # SUBSTANTIATE SUMMARY FILE: path_filename WITH THE CURRENT DIRECTORY AND CSV FILE NAME\n",
    "    smry_data_path_filename = smry_data_path + \"\\\\\" + State_var + '_SWSD_SUMMARY_DATA'\n",
    "\n",
    "    # PROCESSING STATISTICS\n",
    "    start_time_txt = \"The process started at: {}\".format(start_time)\n",
    "    stop_time_txt = \"The process finished at: {}\".format(stop_time)\n",
    "    processing_elapsed_time_txt = \"The processing elapsed time was: {}\".format(processing_elapsed_time)\n",
    "    state_name_txt = \"The State being processed is: {}.\".format(State_var)\n",
    "    csv_cnt_txt = \"The total number of csv files to be processed is: {:,}\".format(csv_count)\n",
    "    state_smry_file_txt = \"The {}_SWSD_SUMMARY_DATA has been written.\".format(State_var)\n",
    "    smry_record_cnt_txt = \"There are {:,} records in the file.\".format(str(len(SWSD)))\n",
    "    new_line = '\\n'\n",
    "\n",
    "    \n",
    "    # WRITE CSV\n",
    "    SWSD.to_csv(smry_data_path_filename +'.csv', header=True, index=False)\n",
    "    \n",
    "    # PRINT PROCESSING STATISTICS TO SCREEN\n",
    "    print(start_time_txt,new_line,stop_time_txt,new_line,processing_elapsed_time_txt,new_line,state_name_txt,\\\n",
    "          new_line,csv_cnt_txt,new_line,smry_record_cnt_txt,new_line,state_smry_file_txt,new_line)\n",
    "    \n",
    "    # CAPTURE PROCESSING STATISTICS FOR TXT FILE\n",
    "    data_reduction = [start_time_txt,stop_time_txt,processing_elapsed_time_txt,state_name_txt,csv_cnt_txt,\\\n",
    "                      state_smry_file_txt,smry_record_cnt_txt,\"\\n\"]\n",
    "    \n",
    "    # MEMORALIZE PROCESSING STATISTICS\n",
    "    with open(swsd_summary_records_path_filename, 'a') as new_file:\n",
    "        for line in data_reduction:\n",
    "            new_file.write(line)\n",
    "            new_file.write('\\n')\n",
    "    \n",
    "    # CLEAR SWSD FILE OF ALL RECORDS\n",
    "    SWSD = SWSD[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
